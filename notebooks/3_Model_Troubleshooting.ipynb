{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "# had to change to tensorflow's keras implementation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = os.pardir\n",
    "in_dir = os.path.join(rootdir, 'data/processed')\n",
    "\n",
    "train_data = np.load(os.path.join(in_dir, 'train.npz'))\n",
    "test_data = np.load(os.path.join(in_dir, 'test.npz'))\n",
    "\n",
    "X_orig = train_data['X_train']\n",
    "Y_orig = train_data['Y_train']\n",
    "X_test_orig = test_data['X_test']\n",
    "Y_test_orig = test_data['Y_test']\n",
    "\n",
    "X_orig = X_orig.reshape(X_orig.shape[0], X_orig.shape[1],\n",
    "                        X_orig.shape[2], 1)\n",
    "X_test_orig = X_test_orig.reshape(X_test_orig.shape[0], X_test_orig.shape[1],\n",
    "                        X_test_orig.shape[2], 1)\n",
    "\n",
    "X_train_orig, X_dev_orig, Y_train_orig, Y_dev_orig = train_test_split(\n",
    "    X_orig, Y_orig, test_size=0.20)\n",
    "\n",
    "X_train = (X_train_orig - X_train_orig.mean()) / X_train_orig.std()\n",
    "X_dev = (X_dev_orig - X_dev_orig.mean()) / X_dev_orig.std()\n",
    "\n",
    "Y_train = Y_train_orig\n",
    "Y_dev = Y_dev_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only define one of the architectures\n",
    "def get_CNN_1A(dropout_rate_conv, dropout_rate_dense):\n",
    "    \"\"\"\n",
    "    The function defines the CNN_1A model\n",
    "    \n",
    "    Arguments:\n",
    "    dropout_rate_conv -- hyperparameter controlling the dropout rate\n",
    "        for conv layers\n",
    "    dropout_rate_dense -- hyperparameter controlling the dropout rate\n",
    "        for dense (fully connected) layers\n",
    "    \n",
    "    Returns:\n",
    "    model -- keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(ZeroPadding2D(padding=(0, 1), input_shape=(11, 34, 1)))\n",
    "    model.add(Conv2D(2, (1, 3), strides=(1, 1),\n",
    "                     activation='relu', use_bias=True,\n",
    "                    kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
    "    model.add(MaxPooling2D(pool_size=(1, 3))) # last column being dropped\n",
    "    model.add(Dropout(dropout_rate_conv))\n",
    "    \n",
    "    model.add(ZeroPadding2D(padding=(1, 1)))\n",
    "    model.add(Conv2D(2, (3, 3), strides=(2, 2),\n",
    "                    activation='relu', use_bias=True,\n",
    "                    kernel_initializer='glorot_normal', bias_initializer='zeros'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(dropout_rate_conv))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate_dense))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(learning_rate):\n",
    "    optimizer = optimizers.Adam(lr=learning_rate) # could use lr decay\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel('loss')\n",
    "    plt.plot(np.arange(1, len(losses)+1), losses)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hyperparameters, custom_metrics, X_train, Y_train, X_dev, Y_dev):\n",
    "    \n",
    "    # get hparams\n",
    "    learning_rate = hyperparameters['lr']\n",
    "    dropout_conv_rate = hyperparameters['dropout_conv']\n",
    "    dropout_dense_rate = hyperparameters['dropout_dense']\n",
    "    n_epochs = hyperparameters['epochs']\n",
    "    get_model = hyperparameters['architecture'][1]\n",
    "\n",
    "    # build model\n",
    "    model = get_model(dropout_conv_rate, dropout_dense_rate)\n",
    "    optimizer = get_optimizer(learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 metrics=custom_metrics,\n",
    "                 loss='binary_crossentropy')\n",
    "    \n",
    "    # train model\n",
    "    history = model.fit(X_train, Y_train, batch_size=128, epochs=n_epochs,\n",
    "              validation_data=(X_dev, Y_dev))\n",
    "    \n",
    "    # plot losses\n",
    "    plot_losses(history.history['loss'])\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics need to be defined according to Keras' standards\n",
    "# using the backend (tensorflow) tensor operations\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    total_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (total_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Computes the F score.\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    \"\"\"\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    f1_score = 2 * (p * r) / (p + r + K.epsilon())\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metrics = ['accuracy', precision, recall, f1_score] # which one to use to evaluate model? accuracy or f1-score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model for ALS classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_als = redefine_labels(Y_train, 1)\n",
    "Y_dev_als = redefine_labels(Y_dev, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters['architecture'] = get_CNN_1A\n",
    "hyperparameters['lr'] = 0.0001\n",
    "hyperparameters['dropout_conv'] = 0\n",
    "hyperparameters['dropout_dense'] = 0\n",
    "hyperparameters['epochs'] = 200\n",
    "\n",
    "model, history = train_model(hyperparameters, custom_metrics,\n",
    "                             X_train, Y_train_als, X_dev, Y_dev_als)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**The model is predicting every case as non ALS!!!!**\n",
    "\n",
    "This happens with each one of the 4 networks, regardless of dropout or learning rate. What is wrong?\n",
    "\n",
    "**Things to try:**\n",
    "1. Bigger networks\n",
    "    * More filters\n",
    "    * More layers\n",
    "2. Different normalization of input data: divide by range instead of standardizing\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Check weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 1, 2)\n",
      "0.78827554\n",
      "-0.8649973\n",
      "0.158396\n"
     ]
    }
   ],
   "source": [
    "W1 = weights[0]\n",
    "print(W1.shape)\n",
    "print(W1.max())\n",
    "print(W1.min())\n",
    "print(W1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "0.09654459\n",
      "0.004111087\n",
      "0.05032784\n"
     ]
    }
   ],
   "source": [
    "b1 = weights[1]\n",
    "print(b1.shape)\n",
    "print(b1.max())\n",
    "print(b1.min())\n",
    "print(b1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2, 2)\n",
      "0.40975013\n",
      "-0.37487692\n",
      "-0.023815565\n"
     ]
    }
   ],
   "source": [
    "W2 = weights[2]\n",
    "print(W2.shape)\n",
    "print(W2.max())\n",
    "print(W2.min())\n",
    "print(W2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "0.07860064\n",
      "-0.06817381\n",
      "0.0052134134\n"
     ]
    }
   ],
   "source": [
    "b2 = weights[3]\n",
    "print(b2.shape)\n",
    "print(b2.max())\n",
    "print(b2.min())\n",
    "print(b2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 12)\n",
      "0.64280665\n",
      "-0.5723054\n",
      "-0.02318577\n"
     ]
    }
   ],
   "source": [
    "W3 = weights[4]\n",
    "print(W3.shape)\n",
    "print(W3.max())\n",
    "print(W3.min())\n",
    "print(W3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,)\n",
      "0.12630284\n",
      "-0.07719131\n",
      "0.010404105\n"
     ]
    }
   ],
   "source": [
    "b3 = weights[5]\n",
    "print(b3.shape)\n",
    "print(b3.max())\n",
    "print(b3.min())\n",
    "print(b3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1)\n",
      "0.3771407\n",
      "-0.6453672\n",
      "-0.099824555\n"
     ]
    }
   ],
   "source": [
    "W4 = weights[6]\n",
    "print(W4.shape)\n",
    "print(W4.max())\n",
    "print(W4.min())\n",
    "print(W4.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "-0.06772911\n",
      "-0.06772911\n",
      "-0.06772911\n"
     ]
    }
   ],
   "source": [
    "b4 = weights[7]\n",
    "print(b4.shape)\n",
    "print(b4.max())\n",
    "print(b4.min())\n",
    "print(b4.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ALS_1A_1 = model\n",
    "history_ALS_1A_1 = history\n",
    "# save it just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Try different normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range in train set: 2357.88922942707\n",
      "Range in dev set: 713.6219252276783\n"
     ]
    }
   ],
   "source": [
    "# range\n",
    "spread_train = (X_train_orig.max() - X_train_orig.min())\n",
    "spread_dev = (X_dev_orig.max() - X_dev_orig.min())\n",
    "print('Range in train set: {0}'.format(spread_train))\n",
    "print('Range in dev set: {0}'.format(spread_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_orig / spread_train\n",
    "X_dev =  X_dev_orig / spread_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 833 samples, validate on 209 samples\n",
      "Epoch 1/200\n",
      "833/833 [==============================] - 1s 642us/step - loss: 4.4986 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3165 - val_loss: 4.7307 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 2/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 4.4496 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 4.6768 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 3/200\n",
      "833/833 [==============================] - 0s 36us/step - loss: 4.3987 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3172 - val_loss: 4.6227 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 4/200\n",
      "833/833 [==============================] - 0s 34us/step - loss: 4.3478 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3166 - val_loss: 4.5690 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 5/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 4.2975 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 4.5157 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 6/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 4.2481 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 4.4629 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 7/200\n",
      "833/833 [==============================] - 0s 35us/step - loss: 4.1988 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3185 - val_loss: 4.4110 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 8/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 4.1505 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3171 - val_loss: 4.3597 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 9/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 4.1023 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3171 - val_loss: 4.3090 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 10/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 4.0550 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3172 - val_loss: 4.2589 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 11/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 4.0083 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 4.2093 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 12/200\n",
      "833/833 [==============================] - 0s 36us/step - loss: 3.9617 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3173 - val_loss: 4.1604 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 13/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 3.9163 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3182 - val_loss: 4.1119 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 14/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 3.8709 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 4.0642 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 15/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 3.8262 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3172 - val_loss: 4.0170 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 16/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 3.7826 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3175 - val_loss: 3.9704 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 17/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 3.7389 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3164 - val_loss: 3.9244 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 18/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 3.6957 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 3.8789 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 19/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 3.6531 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3173 - val_loss: 3.8340 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 20/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 3.6113 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3175 - val_loss: 3.7896 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 21/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 3.5695 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3161 - val_loss: 3.7457 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 22/200\n",
      "833/833 [==============================] - 0s 34us/step - loss: 3.5285 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 3.7020 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 23/200\n",
      "833/833 [==============================] - 0s 27us/step - loss: 3.4874 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3168 - val_loss: 3.6592 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 24/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 3.4473 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3170 - val_loss: 3.6167 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 25/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 3.4076 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3176 - val_loss: 3.5744 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 26/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 3.3680 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3173 - val_loss: 3.5326 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 27/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 3.3287 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 3.4916 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 28/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 3.2903 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3179 - val_loss: 3.4508 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 29/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 3.2520 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3162 - val_loss: 3.4105 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 30/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 3.2144 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 3.3705 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 31/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 3.1771 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3175 - val_loss: 3.3310 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 32/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 3.1400 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3174 - val_loss: 3.2923 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 33/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 3.1037 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3164 - val_loss: 3.2540 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 34/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 3.0677 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3183 - val_loss: 3.2158 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 35/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 3.0321 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3163 - val_loss: 3.1780 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 36/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 2.9967 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 3.1407 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 37/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 2.9617 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3182 - val_loss: 3.1040 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 38/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.9272 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 3.0678 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 39/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 2.8936 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3174 - val_loss: 3.0316 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 40/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.8595 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3167 - val_loss: 2.9963 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 41/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.8264 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3170 - val_loss: 2.9612 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 42/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 2.7936 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3179 - val_loss: 2.9264 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 43/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.7610 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3179 - val_loss: 2.8919 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 44/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.7287 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3173 - val_loss: 2.8580 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 45/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 2.6969 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3172 - val_loss: 2.8244 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 46/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 2.6656 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 2.7910 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 47/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.6344 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3149 - val_loss: 2.7581 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 48/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 2.6039 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3184 - val_loss: 2.7253 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 49/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.5732 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3171 - val_loss: 2.6931 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 50/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 2.5435 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3165 - val_loss: 2.6611 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 51/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.5134 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3179 - val_loss: 2.6297 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 52/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 2.4840 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 2.5987 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 53/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 2.4551 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3183 - val_loss: 2.5679 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 54/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.4265 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3176 - val_loss: 2.5374 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 55/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.3983 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3155 - val_loss: 2.5073 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 56/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 2.3698 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 2.4779 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 57/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.3426 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3184 - val_loss: 2.4485 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 58/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 2.3151 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3166 - val_loss: 2.4194 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 59/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.2879 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3166 - val_loss: 2.3909 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 60/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 2.2614 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 2.3624 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 61/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.2350 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3168 - val_loss: 2.3342 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 62/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 2.2088 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3169 - val_loss: 2.3065 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 63/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 2.1830 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 2.2792 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.1575 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3174 - val_loss: 2.2523 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 65/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.1325 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3174 - val_loss: 2.2256 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 66/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 2.1077 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3176 - val_loss: 2.1994 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 67/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.0831 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 2.1735 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 68/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.0589 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3175 - val_loss: 2.1479 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 69/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 2.0352 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3174 - val_loss: 2.1226 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 70/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 2.0114 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3175 - val_loss: 2.0976 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 71/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.9883 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 2.0727 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 72/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.9651 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3169 - val_loss: 2.0484 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 73/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.9425 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3164 - val_loss: 2.0241 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 74/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 1.9198 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3183 - val_loss: 2.0001 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 75/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.8974 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3171 - val_loss: 1.9765 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 76/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.8754 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3185 - val_loss: 1.9531 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 77/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.8538 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 1.9302 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 78/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.8324 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 1.9075 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 79/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.8113 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 1.8851 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 80/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.7905 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 1.8631 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 81/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.7698 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3186 - val_loss: 1.8415 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 82/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.7497 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 1.8200 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 83/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.7297 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3176 - val_loss: 1.7988 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 84/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.7099 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 1.7781 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 85/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.6906 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 1.7574 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 86/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.6714 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3174 - val_loss: 1.7370 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 87/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.6523 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 1.7169 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 88/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.6336 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 1.6970 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 89/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.6149 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 1.6775 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 90/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.5967 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3175 - val_loss: 1.6583 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 91/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.5788 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 1.6392 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 92/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.5610 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 1.6204 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 93/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.5434 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 1.6020 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 94/200\n",
      "833/833 [==============================] - 0s 27us/step - loss: 1.5261 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3165 - val_loss: 1.5840 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 95/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.5093 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 1.5661 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.4926 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3176 - val_loss: 1.5485 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 97/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.4763 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 1.5312 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 98/200\n",
      "833/833 [==============================] - 0s 27us/step - loss: 1.4602 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3163 - val_loss: 1.5140 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 99/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.4440 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 1.4974 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 100/200\n",
      "833/833 [==============================] - 0s 27us/step - loss: 1.4286 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3168 - val_loss: 1.4808 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 101/200\n",
      "833/833 [==============================] - 0s 27us/step - loss: 1.4132 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3175 - val_loss: 1.4645 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 102/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.3981 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3176 - val_loss: 1.4483 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 103/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.3832 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3163 - val_loss: 1.4324 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 104/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.3683 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3174 - val_loss: 1.4171 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 105/200\n",
      "833/833 [==============================] - 0s 26us/step - loss: 1.3541 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3171 - val_loss: 1.4019 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 106/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.3401 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3162 - val_loss: 1.3867 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 107/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.3261 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3172 - val_loss: 1.3721 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 108/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.3124 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3179 - val_loss: 1.3577 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 109/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 1.2990 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 1.3435 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 110/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.2859 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3182 - val_loss: 1.3295 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 111/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.2729 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3183 - val_loss: 1.3158 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 112/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.2602 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3168 - val_loss: 1.3022 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 113/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.2476 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3184 - val_loss: 1.2889 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 114/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.2353 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 1.2759 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 115/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.2233 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 1.2632 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 116/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.2115 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3144 - val_loss: 1.2506 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 117/200\n",
      "833/833 [==============================] - 0s 27us/step - loss: 1.2000 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3179 - val_loss: 1.2383 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 118/200\n",
      "833/833 [==============================] - 0s 27us/step - loss: 1.1886 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3186 - val_loss: 1.2264 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 119/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.1774 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3183 - val_loss: 1.2146 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 120/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.1665 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 1.2030 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 121/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.1558 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 1.1915 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 122/200\n",
      "833/833 [==============================] - 0s 27us/step - loss: 1.1451 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 1.1803 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 123/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 1.1347 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 1.1692 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 124/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.1244 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 1.1584 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 125/200\n",
      "833/833 [==============================] - 0s 27us/step - loss: 1.1144 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3179 - val_loss: 1.1477 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 126/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.1045 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3180 - val_loss: 1.1373 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 127/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.0949 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3174 - val_loss: 1.1270 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 128/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 1.0854 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3177 - val_loss: 1.1170 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 129/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.0761 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3174 - val_loss: 1.1071 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 130/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 1.0670 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3179 - val_loss: 1.0975 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 131/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 1.0582 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 1.0880 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 132/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.0495 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3182 - val_loss: 1.0787 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 133/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 1.0409 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3186 - val_loss: 1.0696 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 134/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 1.0325 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3186 - val_loss: 1.0607 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 135/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.0242 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3181 - val_loss: 1.0520 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 136/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 1.0161 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3179 - val_loss: 1.0434 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 137/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 1.0082 - acc: 0.1897 - precision: 0.1897 - recall: 1.0000 - f1_score: 0.3178 - val_loss: 1.0349 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 138/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 1.0004 - acc: 0.1885 - precision: 0.1887 - recall: 0.9947 - f1_score: 0.3168 - val_loss: 1.0266 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 139/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.9927 - acc: 0.1873 - precision: 0.1878 - recall: 0.9896 - f1_score: 0.3143 - val_loss: 1.0184 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 140/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.9851 - acc: 0.1861 - precision: 0.1867 - recall: 0.9813 - f1_score: 0.3134 - val_loss: 1.0104 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 141/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.9777 - acc: 0.1849 - precision: 0.1856 - recall: 0.9698 - f1_score: 0.3085 - val_loss: 1.0026 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 142/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.9705 - acc: 0.1849 - precision: 0.1858 - recall: 0.9748 - f1_score: 0.3112 - val_loss: 0.9948 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 143/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.9633 - acc: 0.1849 - precision: 0.1859 - recall: 0.9761 - f1_score: 0.3102 - val_loss: 0.9873 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 144/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 0.9564 - acc: 0.1849 - precision: 0.1858 - recall: 0.9763 - f1_score: 0.3114 - val_loss: 0.9798 - val_acc: 0.1435 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_score: 0.2507\n",
      "Epoch 145/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.9494 - acc: 0.1849 - precision: 0.1858 - recall: 0.9754 - f1_score: 0.3108 - val_loss: 0.9726 - val_acc: 0.1388 - val_precision: 0.1395 - val_recall: 0.9694 - val_f1_score: 0.2435\n",
      "Epoch 146/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.9427 - acc: 0.1849 - precision: 0.1858 - recall: 0.9766 - f1_score: 0.3108 - val_loss: 0.9654 - val_acc: 0.1388 - val_precision: 0.1395 - val_recall: 0.9694 - val_f1_score: 0.2435\n",
      "Epoch 147/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.9361 - acc: 0.1861 - precision: 0.1861 - recall: 0.9757 - f1_score: 0.3108 - val_loss: 0.9583 - val_acc: 0.1388 - val_precision: 0.1395 - val_recall: 0.9694 - val_f1_score: 0.2435\n",
      "Epoch 148/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.9296 - acc: 0.1873 - precision: 0.1862 - recall: 0.9742 - f1_score: 0.3120 - val_loss: 0.9514 - val_acc: 0.1435 - val_precision: 0.1402 - val_recall: 0.9694 - val_f1_score: 0.2446\n",
      "Epoch 149/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.9232 - acc: 0.1897 - precision: 0.1867 - recall: 0.9764 - f1_score: 0.3106 - val_loss: 0.9445 - val_acc: 0.1340 - val_precision: 0.1318 - val_recall: 0.9000 - val_f1_score: 0.2296\n",
      "Epoch 150/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.9168 - acc: 0.1885 - precision: 0.1856 - recall: 0.9662 - f1_score: 0.3112 - val_loss: 0.9378 - val_acc: 0.1340 - val_precision: 0.1318 - val_recall: 0.9000 - val_f1_score: 0.2296\n",
      "Epoch 151/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.9106 - acc: 0.1897 - precision: 0.1859 - recall: 0.9689 - f1_score: 0.3091 - val_loss: 0.9313 - val_acc: 0.1292 - val_precision: 0.1276 - val_recall: 0.8694 - val_f1_score: 0.2221\n",
      "Epoch 152/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.9046 - acc: 0.1909 - precision: 0.1863 - recall: 0.9666 - f1_score: 0.3100 - val_loss: 0.9247 - val_acc: 0.1244 - val_precision: 0.1232 - val_recall: 0.8306 - val_f1_score: 0.2143\n",
      "Epoch 153/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.8986 - acc: 0.1909 - precision: 0.1862 - recall: 0.9713 - f1_score: 0.3107 - val_loss: 0.9184 - val_acc: 0.1244 - val_precision: 0.1232 - val_recall: 0.8306 - val_f1_score: 0.2143\n",
      "Epoch 154/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.8928 - acc: 0.1909 - precision: 0.1863 - recall: 0.9685 - f1_score: 0.3092 - val_loss: 0.9121 - val_acc: 0.1244 - val_precision: 0.1232 - val_recall: 0.8306 - val_f1_score: 0.2143\n",
      "Epoch 155/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.8870 - acc: 0.1921 - precision: 0.1863 - recall: 0.9673 - f1_score: 0.3118 - val_loss: 0.9060 - val_acc: 0.1244 - val_precision: 0.1232 - val_recall: 0.8306 - val_f1_score: 0.2143\n",
      "Epoch 156/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.8814 - acc: 0.1921 - precision: 0.1856 - recall: 0.9616 - f1_score: 0.3106 - val_loss: 0.8999 - val_acc: 0.1244 - val_precision: 0.1232 - val_recall: 0.8306 - val_f1_score: 0.2143\n",
      "Epoch 157/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.8758 - acc: 0.1921 - precision: 0.1841 - recall: 0.9510 - f1_score: 0.3067 - val_loss: 0.8940 - val_acc: 0.1244 - val_precision: 0.1232 - val_recall: 0.8306 - val_f1_score: 0.2143\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833/833 [==============================] - 0s 31us/step - loss: 0.8704 - acc: 0.1969 - precision: 0.1849 - recall: 0.9496 - f1_score: 0.3080 - val_loss: 0.8882 - val_acc: 0.1196 - val_precision: 0.1187 - val_recall: 0.7919 - val_f1_score: 0.2063\n",
      "Epoch 159/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.8651 - acc: 0.1969 - precision: 0.1850 - recall: 0.9537 - f1_score: 0.3082 - val_loss: 0.8824 - val_acc: 0.1196 - val_precision: 0.1149 - val_recall: 0.7612 - val_f1_score: 0.1994\n",
      "Epoch 160/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.8598 - acc: 0.1981 - precision: 0.1844 - recall: 0.9476 - f1_score: 0.3069 - val_loss: 0.8768 - val_acc: 0.1196 - val_precision: 0.1149 - val_recall: 0.7612 - val_f1_score: 0.1994\n",
      "Epoch 161/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.8546 - acc: 0.2005 - precision: 0.1843 - recall: 0.9349 - f1_score: 0.3070 - val_loss: 0.8713 - val_acc: 0.1244 - val_precision: 0.1156 - val_recall: 0.7612 - val_f1_score: 0.2004\n",
      "Epoch 162/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.8496 - acc: 0.2029 - precision: 0.1845 - recall: 0.9331 - f1_score: 0.3073 - val_loss: 0.8657 - val_acc: 0.1196 - val_precision: 0.1112 - val_recall: 0.7306 - val_f1_score: 0.1927\n",
      "Epoch 163/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.8446 - acc: 0.2041 - precision: 0.1845 - recall: 0.9350 - f1_score: 0.3077 - val_loss: 0.8603 - val_acc: 0.1196 - val_precision: 0.1112 - val_recall: 0.7306 - val_f1_score: 0.1927\n",
      "Epoch 164/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.8396 - acc: 0.2065 - precision: 0.1844 - recall: 0.9291 - f1_score: 0.3073 - val_loss: 0.8550 - val_acc: 0.1292 - val_precision: 0.1124 - val_recall: 0.7306 - val_f1_score: 0.1945\n",
      "Epoch 165/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 0.8348 - acc: 0.2065 - precision: 0.1848 - recall: 0.9357 - f1_score: 0.3068 - val_loss: 0.8498 - val_acc: 0.1340 - val_precision: 0.1129 - val_recall: 0.7306 - val_f1_score: 0.1953\n",
      "Epoch 166/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.8301 - acc: 0.2077 - precision: 0.1838 - recall: 0.9210 - f1_score: 0.3045 - val_loss: 0.8448 - val_acc: 0.1531 - val_precision: 0.1154 - val_recall: 0.7306 - val_f1_score: 0.1989\n",
      "Epoch 167/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.8255 - acc: 0.2101 - precision: 0.1839 - recall: 0.9170 - f1_score: 0.3041 - val_loss: 0.8398 - val_acc: 0.1531 - val_precision: 0.1154 - val_recall: 0.7306 - val_f1_score: 0.1989\n",
      "Epoch 168/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.8209 - acc: 0.2185 - precision: 0.1852 - recall: 0.9233 - f1_score: 0.3059 - val_loss: 0.8349 - val_acc: 0.1579 - val_precision: 0.1159 - val_recall: 0.7306 - val_f1_score: 0.1997\n",
      "Epoch 169/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.8164 - acc: 0.2173 - precision: 0.1830 - recall: 0.9053 - f1_score: 0.3042 - val_loss: 0.8300 - val_acc: 0.1579 - val_precision: 0.1159 - val_recall: 0.7306 - val_f1_score: 0.1997\n",
      "Epoch 170/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.8119 - acc: 0.2221 - precision: 0.1836 - recall: 0.8990 - f1_score: 0.3034 - val_loss: 0.8253 - val_acc: 0.1579 - val_precision: 0.1159 - val_recall: 0.7306 - val_f1_score: 0.1997\n",
      "Epoch 171/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.8076 - acc: 0.2293 - precision: 0.1851 - recall: 0.9036 - f1_score: 0.3062 - val_loss: 0.8206 - val_acc: 0.1675 - val_precision: 0.1170 - val_recall: 0.7306 - val_f1_score: 0.2015\n",
      "Epoch 172/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.8033 - acc: 0.2329 - precision: 0.1863 - recall: 0.9018 - f1_score: 0.3069 - val_loss: 0.8160 - val_acc: 0.1675 - val_precision: 0.1170 - val_recall: 0.7306 - val_f1_score: 0.2015\n",
      "Epoch 173/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.7991 - acc: 0.2377 - precision: 0.1845 - recall: 0.8870 - f1_score: 0.3047 - val_loss: 0.8115 - val_acc: 0.1722 - val_precision: 0.1177 - val_recall: 0.7306 - val_f1_score: 0.2025\n",
      "Epoch 174/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.7950 - acc: 0.2449 - precision: 0.1862 - recall: 0.8751 - f1_score: 0.3053 - val_loss: 0.8071 - val_acc: 0.1770 - val_precision: 0.1183 - val_recall: 0.7306 - val_f1_score: 0.2033\n",
      "Epoch 175/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.7909 - acc: 0.2497 - precision: 0.1861 - recall: 0.8736 - f1_score: 0.3055 - val_loss: 0.8026 - val_acc: 0.1866 - val_precision: 0.1197 - val_recall: 0.7306 - val_f1_score: 0.2054\n",
      "Epoch 176/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.7868 - acc: 0.2557 - precision: 0.1863 - recall: 0.8734 - f1_score: 0.3049 - val_loss: 0.7984 - val_acc: 0.1962 - val_precision: 0.1210 - val_recall: 0.7306 - val_f1_score: 0.2072\n",
      "Epoch 177/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.7829 - acc: 0.2545 - precision: 0.1842 - recall: 0.8561 - f1_score: 0.3016 - val_loss: 0.7941 - val_acc: 0.2010 - val_precision: 0.1217 - val_recall: 0.7306 - val_f1_score: 0.2083\n",
      "Epoch 178/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.7790 - acc: 0.2581 - precision: 0.1838 - recall: 0.8495 - f1_score: 0.3013 - val_loss: 0.7899 - val_acc: 0.2057 - val_precision: 0.1183 - val_recall: 0.7000 - val_f1_score: 0.2020\n",
      "Epoch 179/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.7752 - acc: 0.2617 - precision: 0.1841 - recall: 0.8458 - f1_score: 0.3016 - val_loss: 0.7858 - val_acc: 0.2010 - val_precision: 0.1133 - val_recall: 0.6694 - val_f1_score: 0.1934\n",
      "Epoch 180/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.7714 - acc: 0.2689 - precision: 0.1819 - recall: 0.8247 - f1_score: 0.2954 - val_loss: 0.7818 - val_acc: 0.2201 - val_precision: 0.1159 - val_recall: 0.6694 - val_f1_score: 0.1971\n",
      "Epoch 181/200\n",
      "833/833 [==============================] - 0s 34us/step - loss: 0.7678 - acc: 0.2737 - precision: 0.1820 - recall: 0.8128 - f1_score: 0.2963 - val_loss: 0.7779 - val_acc: 0.2297 - val_precision: 0.1128 - val_recall: 0.6388 - val_f1_score: 0.1912\n",
      "Epoch 182/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.7641 - acc: 0.2797 - precision: 0.1819 - recall: 0.7995 - f1_score: 0.2952 - val_loss: 0.7739 - val_acc: 0.2392 - val_precision: 0.1141 - val_recall: 0.6388 - val_f1_score: 0.1931\n",
      "Epoch 183/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.7605 - acc: 0.2857 - precision: 0.1793 - recall: 0.7727 - f1_score: 0.2909 - val_loss: 0.7701 - val_acc: 0.2488 - val_precision: 0.1157 - val_recall: 0.6388 - val_f1_score: 0.1952\n",
      "Epoch 184/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.7571 - acc: 0.2893 - precision: 0.1788 - recall: 0.7596 - f1_score: 0.2871 - val_loss: 0.7664 - val_acc: 0.2632 - val_precision: 0.1179 - val_recall: 0.6388 - val_f1_score: 0.1983\n",
      "Epoch 185/200\n",
      "833/833 [==============================] - 0s 34us/step - loss: 0.7536 - acc: 0.2953 - precision: 0.1766 - recall: 0.7445 - f1_score: 0.2851 - val_loss: 0.7627 - val_acc: 0.2775 - val_precision: 0.1201 - val_recall: 0.6388 - val_f1_score: 0.2015\n",
      "Epoch 186/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.7502 - acc: 0.3109 - precision: 0.1804 - recall: 0.7468 - f1_score: 0.2903 - val_loss: 0.7591 - val_acc: 0.2823 - val_precision: 0.1210 - val_recall: 0.6388 - val_f1_score: 0.2026\n",
      "Epoch 187/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.7469 - acc: 0.3265 - precision: 0.1833 - recall: 0.7338 - f1_score: 0.2926 - val_loss: 0.7556 - val_acc: 0.2967 - val_precision: 0.1229 - val_recall: 0.6388 - val_f1_score: 0.2056\n",
      "Epoch 188/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.7437 - acc: 0.3397 - precision: 0.1858 - recall: 0.7344 - f1_score: 0.2952 - val_loss: 0.7521 - val_acc: 0.3254 - val_precision: 0.1282 - val_recall: 0.6388 - val_f1_score: 0.2127\n",
      "Epoch 189/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 0.7405 - acc: 0.3505 - precision: 0.1886 - recall: 0.7329 - f1_score: 0.2991 - val_loss: 0.7486 - val_acc: 0.3349 - val_precision: 0.1299 - val_recall: 0.6388 - val_f1_score: 0.2151\n",
      "Epoch 190/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.7373 - acc: 0.3601 - precision: 0.1907 - recall: 0.7286 - f1_score: 0.3015 - val_loss: 0.7452 - val_acc: 0.3445 - val_precision: 0.1320 - val_recall: 0.6388 - val_f1_score: 0.2178\n",
      "Epoch 191/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.7342 - acc: 0.3709 - precision: 0.1936 - recall: 0.7246 - f1_score: 0.3044 - val_loss: 0.7419 - val_acc: 0.3589 - val_precision: 0.1348 - val_recall: 0.6388 - val_f1_score: 0.2216\n",
      "Epoch 192/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.7311 - acc: 0.3782 - precision: 0.1916 - recall: 0.7053 - f1_score: 0.3007 - val_loss: 0.7386 - val_acc: 0.3636 - val_precision: 0.1305 - val_recall: 0.6081 - val_f1_score: 0.2136\n",
      "Epoch 193/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.7281 - acc: 0.3842 - precision: 0.1891 - recall: 0.6804 - f1_score: 0.2949 - val_loss: 0.7353 - val_acc: 0.3828 - val_precision: 0.1343 - val_recall: 0.6081 - val_f1_score: 0.2187\n",
      "Epoch 194/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.7251 - acc: 0.3974 - precision: 0.1906 - recall: 0.6680 - f1_score: 0.2946 - val_loss: 0.7321 - val_acc: 0.4019 - val_precision: 0.1329 - val_recall: 0.5694 - val_f1_score: 0.2145\n",
      "Epoch 195/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.7222 - acc: 0.4082 - precision: 0.1917 - recall: 0.6601 - f1_score: 0.2958 - val_loss: 0.7290 - val_acc: 0.4115 - val_precision: 0.1353 - val_recall: 0.5694 - val_f1_score: 0.2175\n",
      "Epoch 196/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.7193 - acc: 0.4190 - precision: 0.1950 - recall: 0.6450 - f1_score: 0.2963 - val_loss: 0.7260 - val_acc: 0.4306 - val_precision: 0.1401 - val_recall: 0.5694 - val_f1_score: 0.2235\n",
      "Epoch 197/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.7166 - acc: 0.4298 - precision: 0.1923 - recall: 0.6206 - f1_score: 0.2920 - val_loss: 0.7230 - val_acc: 0.4498 - val_precision: 0.1386 - val_recall: 0.5306 - val_f1_score: 0.2188\n",
      "Epoch 198/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.7138 - acc: 0.4478 - precision: 0.1984 - recall: 0.6228 - f1_score: 0.2988 - val_loss: 0.7200 - val_acc: 0.4498 - val_precision: 0.1318 - val_recall: 0.5000 - val_f1_score: 0.2076\n",
      "Epoch 199/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.7111 - acc: 0.4634 - precision: 0.2006 - recall: 0.6091 - f1_score: 0.3015 - val_loss: 0.7171 - val_acc: 0.4545 - val_precision: 0.1332 - val_recall: 0.5000 - val_f1_score: 0.2092\n",
      "Epoch 200/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.7084 - acc: 0.4754 - precision: 0.2051 - recall: 0.6315 - f1_score: 0.3069 - val_loss: 0.7142 - val_acc: 0.4545 - val_precision: 0.1332 - val_recall: 0.5000 - val_f1_score: 0.2092\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperparameters['architecture'] = get_CNN_1A\n",
    "hyperparameters['lr'] = 0.0001\n",
    "hyperparameters['dropout_conv'] = 0\n",
    "hyperparameters['dropout_dense'] = 0\n",
    "hyperparameters['epochs'] = 200\n",
    "\n",
    "model, history = train_model(hyperparameters, custom_metrics,\n",
    "                             X_train, Y_train_als, X_dev, Y_dev_als)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) Try with bigger NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to original normalization\n",
    "X_train = (X_train_orig - X_train_orig.mean()) / X_train_orig.std()\n",
    "X_dev = (X_dev_orig - X_dev_orig.mean()) / X_dev_orig.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 833 samples, validate on 209 samples\n",
      "Epoch 1/200\n",
      "833/833 [==============================] - 1s 791us/step - loss: 0.6839 - acc: 0.7251 - precision: 0.4164 - recall: 0.7007 - f1_score: 0.4942 - val_loss: 0.6711 - val_acc: 0.8373 - val_precision: 0.3742 - val_recall: 0.2225 - val_f1_score: 0.2696\n",
      "Epoch 2/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.6634 - acc: 0.8079 - precision: 0.2878 - recall: 0.0638 - f1_score: 0.0996 - val_loss: 0.6470 - val_acc: 0.8612 - val_precision: 0.6124 - val_recall: 0.0306 - val_f1_score: 0.0583\n",
      "Epoch 3/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.6437 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.6237 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/200\n",
      "833/833 [==============================] - 0s 38us/step - loss: 0.6244 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.6016 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/200\n",
      "833/833 [==============================] - 0s 38us/step - loss: 0.6071 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.5804 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/200\n",
      "833/833 [==============================] - 0s 35us/step - loss: 0.5903 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.5601 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/200\n",
      "833/833 [==============================] - 0s 34us/step - loss: 0.5742 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.5409 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.5595 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.5226 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/200\n",
      "833/833 [==============================] - 0s 36us/step - loss: 0.5453 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.5056 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/200\n",
      "833/833 [==============================] - 0s 35us/step - loss: 0.5329 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4900 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 11/200\n",
      "833/833 [==============================] - 0s 34us/step - loss: 0.5211 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4756 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 12/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.5108 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4624 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 13/200\n",
      "833/833 [==============================] - 0s 37us/step - loss: 0.5014 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4502 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 14/200\n",
      "833/833 [==============================] - 0s 36us/step - loss: 0.4934 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4397 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 15/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.4862 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4314 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 16/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.4809 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4248 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 17/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.4768 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4198 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 18/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.4737 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4156 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 19/200\n",
      "833/833 [==============================] - 0s 36us/step - loss: 0.4707 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4126 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 20/200\n",
      "833/833 [==============================] - 0s 35us/step - loss: 0.4680 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4100 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 21/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.4656 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4076 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 22/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.4631 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4055 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 23/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.4606 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4036 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 24/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4583 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4018 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 25/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.4557 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.4006 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 26/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.4534 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3988 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 27/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.4507 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3974 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 28/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.4483 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3963 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 29/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4457 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3953 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4431 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3939 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 31/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4404 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3925 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 32/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4377 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3909 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 33/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.4350 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3895 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 34/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4320 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3873 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 35/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.4292 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3847 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 36/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.4261 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3831 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 37/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4229 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3823 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 38/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4197 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3812 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 39/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.4164 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3789 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 40/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4127 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3778 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 41/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.4092 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3767 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 42/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.4061 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3753 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 43/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.4026 - acc: 0.8103 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1_score: 0.0000e+00 - val_loss: 0.3730 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 44/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3993 - acc: 0.8115 - precision: 0.1537 - recall: 0.0061 - f1_score: 0.0118 - val_loss: 0.3704 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 45/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3962 - acc: 0.8139 - precision: 0.3854 - recall: 0.0186 - f1_score: 0.0353 - val_loss: 0.3688 - val_acc: 0.8565 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n",
      "Epoch 46/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3930 - acc: 0.8163 - precision: 0.3854 - recall: 0.0308 - f1_score: 0.0558 - val_loss: 0.3669 - val_acc: 0.8660 - val_precision: 0.6124 - val_recall: 0.0612 - val_f1_score: 0.1114\n",
      "Epoch 47/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3898 - acc: 0.8223 - precision: 0.9220 - recall: 0.0595 - f1_score: 0.1113 - val_loss: 0.3658 - val_acc: 0.8660 - val_precision: 0.6124 - val_recall: 0.0612 - val_f1_score: 0.1114\n",
      "Epoch 48/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3868 - acc: 0.8271 - precision: 0.9220 - recall: 0.0832 - f1_score: 0.1507 - val_loss: 0.3645 - val_acc: 0.8708 - val_precision: 0.6124 - val_recall: 0.0919 - val_f1_score: 0.1598\n",
      "Epoch 49/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3842 - acc: 0.8295 - precision: 1.0000 - recall: 0.0989 - f1_score: 0.1758 - val_loss: 0.3635 - val_acc: 0.8708 - val_precision: 0.6124 - val_recall: 0.0919 - val_f1_score: 0.1598\n",
      "Epoch 50/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3812 - acc: 0.8355 - precision: 1.0000 - recall: 0.1327 - f1_score: 0.2283 - val_loss: 0.3611 - val_acc: 0.8708 - val_precision: 0.6124 - val_recall: 0.0919 - val_f1_score: 0.1598\n",
      "Epoch 51/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3784 - acc: 0.8415 - precision: 1.0000 - recall: 0.1704 - f1_score: 0.2824 - val_loss: 0.3597 - val_acc: 0.8756 - val_precision: 0.6124 - val_recall: 0.1225 - val_f1_score: 0.2041\n",
      "Epoch 52/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3760 - acc: 0.8451 - precision: 1.0000 - recall: 0.1993 - f1_score: 0.3219 - val_loss: 0.3581 - val_acc: 0.8852 - val_precision: 1.0000 - val_recall: 0.1919 - val_f1_score: 0.3154\n",
      "Epoch 53/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3736 - acc: 0.8499 - precision: 0.9524 - recall: 0.2266 - f1_score: 0.3593 - val_loss: 0.3590 - val_acc: 0.8900 - val_precision: 1.0000 - val_recall: 0.2306 - val_f1_score: 0.3742\n",
      "Epoch 54/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3712 - acc: 0.8547 - precision: 0.9078 - recall: 0.2390 - f1_score: 0.3764 - val_loss: 0.3569 - val_acc: 0.8852 - val_precision: 0.8979 - val_recall: 0.2306 - val_f1_score: 0.3647\n",
      "Epoch 55/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3687 - acc: 0.8559 - precision: 0.9396 - recall: 0.2619 - f1_score: 0.4048 - val_loss: 0.3559 - val_acc: 0.8804 - val_precision: 0.7687 - val_recall: 0.2306 - val_f1_score: 0.3548\n",
      "Epoch 56/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3666 - acc: 0.8559 - precision: 0.8733 - recall: 0.2745 - f1_score: 0.4115 - val_loss: 0.3557 - val_acc: 0.8804 - val_precision: 0.7687 - val_recall: 0.2306 - val_f1_score: 0.3548\n",
      "Epoch 57/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3644 - acc: 0.8535 - precision: 0.8549 - recall: 0.2754 - f1_score: 0.4011 - val_loss: 0.3536 - val_acc: 0.8804 - val_precision: 0.7687 - val_recall: 0.2306 - val_f1_score: 0.3548\n",
      "Epoch 58/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3623 - acc: 0.8547 - precision: 0.8246 - recall: 0.3076 - f1_score: 0.4340 - val_loss: 0.3527 - val_acc: 0.8804 - val_precision: 0.7177 - val_recall: 0.2612 - val_f1_score: 0.3817\n",
      "Epoch 59/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3604 - acc: 0.8547 - precision: 0.8142 - recall: 0.3156 - f1_score: 0.4470 - val_loss: 0.3523 - val_acc: 0.8804 - val_precision: 0.7177 - val_recall: 0.2612 - val_f1_score: 0.3817\n",
      "Epoch 60/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3585 - acc: 0.8535 - precision: 0.7650 - recall: 0.3116 - f1_score: 0.4354 - val_loss: 0.3520 - val_acc: 0.8756 - val_precision: 0.6667 - val_recall: 0.2612 - val_f1_score: 0.3727\n",
      "Epoch 61/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3568 - acc: 0.8523 - precision: 0.7494 - recall: 0.3100 - f1_score: 0.4359 - val_loss: 0.3506 - val_acc: 0.8756 - val_precision: 0.6667 - val_recall: 0.2612 - val_f1_score: 0.3727\n",
      "Epoch 62/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3551 - acc: 0.8535 - precision: 0.7582 - recall: 0.3289 - f1_score: 0.4482 - val_loss: 0.3506 - val_acc: 0.8708 - val_precision: 0.6258 - val_recall: 0.2612 - val_f1_score: 0.3642\n",
      "Epoch 63/200\n",
      "833/833 [==============================] - 0s 35us/step - loss: 0.3536 - acc: 0.8535 - precision: 0.7277 - recall: 0.3362 - f1_score: 0.4537 - val_loss: 0.3495 - val_acc: 0.8660 - val_precision: 0.5924 - val_recall: 0.2612 - val_f1_score: 0.3563\n",
      "Epoch 64/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3519 - acc: 0.8547 - precision: 0.7622 - recall: 0.3543 - f1_score: 0.4780 - val_loss: 0.3501 - val_acc: 0.8660 - val_precision: 0.5881 - val_recall: 0.2919 - val_f1_score: 0.3791\n",
      "Epoch 65/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3507 - acc: 0.8535 - precision: 0.7002 - recall: 0.3488 - f1_score: 0.4627 - val_loss: 0.3487 - val_acc: 0.8660 - val_precision: 0.5881 - val_recall: 0.2919 - val_f1_score: 0.3791\n",
      "Epoch 66/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3493 - acc: 0.8535 - precision: 0.7365 - recall: 0.3778 - f1_score: 0.4927 - val_loss: 0.3491 - val_acc: 0.8660 - val_precision: 0.5881 - val_recall: 0.2919 - val_f1_score: 0.3791\n",
      "Epoch 67/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3479 - acc: 0.8571 - precision: 0.7561 - recall: 0.3775 - f1_score: 0.4978 - val_loss: 0.3466 - val_acc: 0.8660 - val_precision: 0.5881 - val_recall: 0.2919 - val_f1_score: 0.3791\n",
      "Epoch 68/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3468 - acc: 0.8559 - precision: 0.7537 - recall: 0.3727 - f1_score: 0.4884 - val_loss: 0.3456 - val_acc: 0.8660 - val_precision: 0.5881 - val_recall: 0.2919 - val_f1_score: 0.3791\n",
      "Epoch 69/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3457 - acc: 0.8547 - precision: 0.7303 - recall: 0.3904 - f1_score: 0.5032 - val_loss: 0.3464 - val_acc: 0.8612 - val_precision: 0.5236 - val_recall: 0.2919 - val_f1_score: 0.3706\n",
      "Epoch 70/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.3445 - acc: 0.8571 - precision: 0.7361 - recall: 0.3940 - f1_score: 0.5011 - val_loss: 0.3466 - val_acc: 0.8612 - val_precision: 0.5236 - val_recall: 0.2919 - val_f1_score: 0.3706\n",
      "Epoch 71/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.3436 - acc: 0.8571 - precision: 0.7131 - recall: 0.3968 - f1_score: 0.5076 - val_loss: 0.3468 - val_acc: 0.8612 - val_precision: 0.5236 - val_recall: 0.2919 - val_f1_score: 0.3706\n",
      "Epoch 72/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3426 - acc: 0.8559 - precision: 0.7309 - recall: 0.3974 - f1_score: 0.5116 - val_loss: 0.3459 - val_acc: 0.8612 - val_precision: 0.5236 - val_recall: 0.2919 - val_f1_score: 0.3706\n",
      "Epoch 73/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3418 - acc: 0.8547 - precision: 0.7038 - recall: 0.4087 - f1_score: 0.5057 - val_loss: 0.3457 - val_acc: 0.8612 - val_precision: 0.5236 - val_recall: 0.2919 - val_f1_score: 0.3706\n",
      "Epoch 74/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.3412 - acc: 0.8571 - precision: 0.7003 - recall: 0.3944 - f1_score: 0.5001 - val_loss: 0.3424 - val_acc: 0.8612 - val_precision: 0.5236 - val_recall: 0.2919 - val_f1_score: 0.3706\n",
      "Epoch 75/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3403 - acc: 0.8583 - precision: 0.7344 - recall: 0.4073 - f1_score: 0.5213 - val_loss: 0.3416 - val_acc: 0.8612 - val_precision: 0.5236 - val_recall: 0.2919 - val_f1_score: 0.3706\n",
      "Epoch 76/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.3395 - acc: 0.8583 - precision: 0.7215 - recall: 0.3866 - f1_score: 0.4998 - val_loss: 0.3430 - val_acc: 0.8612 - val_precision: 0.5236 - val_recall: 0.2919 - val_f1_score: 0.3706\n",
      "Epoch 77/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3386 - acc: 0.8535 - precision: 0.7161 - recall: 0.3946 - f1_score: 0.5047 - val_loss: 0.3441 - val_acc: 0.8612 - val_precision: 0.5236 - val_recall: 0.2919 - val_f1_score: 0.3706\n",
      "Epoch 78/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3379 - acc: 0.8547 - precision: 0.6934 - recall: 0.4044 - f1_score: 0.5068 - val_loss: 0.3437 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 79/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3372 - acc: 0.8547 - precision: 0.6826 - recall: 0.4043 - f1_score: 0.5070 - val_loss: 0.3442 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 80/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3367 - acc: 0.8559 - precision: 0.6958 - recall: 0.4128 - f1_score: 0.5135 - val_loss: 0.3431 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 81/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.3360 - acc: 0.8547 - precision: 0.6933 - recall: 0.4150 - f1_score: 0.5130 - val_loss: 0.3430 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 82/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3356 - acc: 0.8547 - precision: 0.6881 - recall: 0.4120 - f1_score: 0.5084 - val_loss: 0.3420 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 83/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3347 - acc: 0.8559 - precision: 0.7032 - recall: 0.4189 - f1_score: 0.5191 - val_loss: 0.3426 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 84/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3343 - acc: 0.8535 - precision: 0.6742 - recall: 0.4261 - f1_score: 0.5150 - val_loss: 0.3429 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 85/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3339 - acc: 0.8547 - precision: 0.7095 - recall: 0.4490 - f1_score: 0.5408 - val_loss: 0.3426 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 86/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 0.3331 - acc: 0.8535 - precision: 0.6823 - recall: 0.4153 - f1_score: 0.5123 - val_loss: 0.3397 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 87/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3327 - acc: 0.8547 - precision: 0.6839 - recall: 0.3968 - f1_score: 0.4982 - val_loss: 0.3391 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 88/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3324 - acc: 0.8547 - precision: 0.6881 - recall: 0.3981 - f1_score: 0.4953 - val_loss: 0.3385 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 89/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3317 - acc: 0.8547 - precision: 0.6996 - recall: 0.4174 - f1_score: 0.5174 - val_loss: 0.3393 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 90/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3312 - acc: 0.8535 - precision: 0.6761 - recall: 0.4122 - f1_score: 0.5116 - val_loss: 0.3396 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 91/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3308 - acc: 0.8535 - precision: 0.7117 - recall: 0.4122 - f1_score: 0.5078 - val_loss: 0.3392 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3303 - acc: 0.8535 - precision: 0.6772 - recall: 0.4000 - f1_score: 0.4967 - val_loss: 0.3383 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 93/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3299 - acc: 0.8535 - precision: 0.6978 - recall: 0.4278 - f1_score: 0.5253 - val_loss: 0.3381 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 94/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3295 - acc: 0.8535 - precision: 0.6850 - recall: 0.4163 - f1_score: 0.5162 - val_loss: 0.3379 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 95/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3291 - acc: 0.8535 - precision: 0.6690 - recall: 0.4190 - f1_score: 0.5076 - val_loss: 0.3379 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 96/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3287 - acc: 0.8499 - precision: 0.6710 - recall: 0.4130 - f1_score: 0.5035 - val_loss: 0.3399 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 97/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3287 - acc: 0.8511 - precision: 0.6670 - recall: 0.4307 - f1_score: 0.5163 - val_loss: 0.3381 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 98/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3280 - acc: 0.8511 - precision: 0.6530 - recall: 0.4283 - f1_score: 0.5062 - val_loss: 0.3379 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 99/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3275 - acc: 0.8511 - precision: 0.6739 - recall: 0.4352 - f1_score: 0.5254 - val_loss: 0.3373 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 100/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 0.3273 - acc: 0.8499 - precision: 0.6705 - recall: 0.4370 - f1_score: 0.5286 - val_loss: 0.3387 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 101/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3267 - acc: 0.8499 - precision: 0.6663 - recall: 0.4239 - f1_score: 0.5145 - val_loss: 0.3356 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 102/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3263 - acc: 0.8511 - precision: 0.6687 - recall: 0.4326 - f1_score: 0.5186 - val_loss: 0.3350 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 103/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3259 - acc: 0.8499 - precision: 0.6639 - recall: 0.4138 - f1_score: 0.5068 - val_loss: 0.3348 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 104/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 0.3256 - acc: 0.8511 - precision: 0.6750 - recall: 0.4205 - f1_score: 0.5143 - val_loss: 0.3333 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 105/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3252 - acc: 0.8511 - precision: 0.6770 - recall: 0.4175 - f1_score: 0.5128 - val_loss: 0.3335 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 106/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3249 - acc: 0.8499 - precision: 0.6653 - recall: 0.4273 - f1_score: 0.5042 - val_loss: 0.3338 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 107/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3245 - acc: 0.8499 - precision: 0.6445 - recall: 0.4153 - f1_score: 0.5027 - val_loss: 0.3332 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 108/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3240 - acc: 0.8499 - precision: 0.6673 - recall: 0.4205 - f1_score: 0.5087 - val_loss: 0.3340 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 109/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3239 - acc: 0.8499 - precision: 0.6645 - recall: 0.4235 - f1_score: 0.5139 - val_loss: 0.3328 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 110/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3233 - acc: 0.8499 - precision: 0.6635 - recall: 0.4228 - f1_score: 0.5101 - val_loss: 0.3334 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 111/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3231 - acc: 0.8511 - precision: 0.6577 - recall: 0.4320 - f1_score: 0.5208 - val_loss: 0.3340 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 112/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3229 - acc: 0.8511 - precision: 0.6904 - recall: 0.4466 - f1_score: 0.5341 - val_loss: 0.3320 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 113/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3222 - acc: 0.8499 - precision: 0.6664 - recall: 0.4223 - f1_score: 0.5147 - val_loss: 0.3331 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 114/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3222 - acc: 0.8523 - precision: 0.6648 - recall: 0.4474 - f1_score: 0.5268 - val_loss: 0.3336 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 115/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3217 - acc: 0.8523 - precision: 0.6370 - recall: 0.4382 - f1_score: 0.5074 - val_loss: 0.3319 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 116/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3214 - acc: 0.8523 - precision: 0.6711 - recall: 0.4494 - f1_score: 0.5263 - val_loss: 0.3308 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 117/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3210 - acc: 0.8511 - precision: 0.6682 - recall: 0.4331 - f1_score: 0.5187 - val_loss: 0.3303 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 118/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3207 - acc: 0.8499 - precision: 0.6589 - recall: 0.4273 - f1_score: 0.5142 - val_loss: 0.3310 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 119/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3205 - acc: 0.8523 - precision: 0.6686 - recall: 0.4552 - f1_score: 0.5361 - val_loss: 0.3307 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 120/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3198 - acc: 0.8511 - precision: 0.6567 - recall: 0.4357 - f1_score: 0.5186 - val_loss: 0.3288 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 121/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3195 - acc: 0.8499 - precision: 0.6631 - recall: 0.4253 - f1_score: 0.5164 - val_loss: 0.3262 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 122/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3195 - acc: 0.8487 - precision: 0.6615 - recall: 0.4342 - f1_score: 0.5171 - val_loss: 0.3264 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 123/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3193 - acc: 0.8511 - precision: 0.6555 - recall: 0.4361 - f1_score: 0.5193 - val_loss: 0.3287 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 124/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3193 - acc: 0.8499 - precision: 0.6641 - recall: 0.4216 - f1_score: 0.5054 - val_loss: 0.3263 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 125/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3182 - acc: 0.8511 - precision: 0.6798 - recall: 0.4249 - f1_score: 0.5147 - val_loss: 0.3282 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 126/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3182 - acc: 0.8523 - precision: 0.6565 - recall: 0.4386 - f1_score: 0.5227 - val_loss: 0.3299 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 127/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3178 - acc: 0.8523 - precision: 0.6809 - recall: 0.4637 - f1_score: 0.5409 - val_loss: 0.3276 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 128/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3180 - acc: 0.8523 - precision: 0.6789 - recall: 0.4441 - f1_score: 0.5208 - val_loss: 0.3245 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 129/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3173 - acc: 0.8511 - precision: 0.6581 - recall: 0.4460 - f1_score: 0.5197 - val_loss: 0.3262 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 130/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3172 - acc: 0.8499 - precision: 0.6445 - recall: 0.4308 - f1_score: 0.5125 - val_loss: 0.3241 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 131/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3165 - acc: 0.8511 - precision: 0.6573 - recall: 0.4387 - f1_score: 0.5188 - val_loss: 0.3248 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 132/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3160 - acc: 0.8511 - precision: 0.6665 - recall: 0.4291 - f1_score: 0.5156 - val_loss: 0.3261 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 133/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3159 - acc: 0.8523 - precision: 0.6694 - recall: 0.4467 - f1_score: 0.5287 - val_loss: 0.3266 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 134/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3155 - acc: 0.8499 - precision: 0.6511 - recall: 0.4337 - f1_score: 0.5178 - val_loss: 0.3255 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 135/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3152 - acc: 0.8511 - precision: 0.6710 - recall: 0.4350 - f1_score: 0.5227 - val_loss: 0.3230 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 136/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.3149 - acc: 0.8499 - precision: 0.6715 - recall: 0.4452 - f1_score: 0.5190 - val_loss: 0.3218 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 137/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3148 - acc: 0.8511 - precision: 0.6520 - recall: 0.4222 - f1_score: 0.5073 - val_loss: 0.3241 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 138/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3141 - acc: 0.8511 - precision: 0.6793 - recall: 0.4358 - f1_score: 0.5240 - val_loss: 0.3215 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 139/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3139 - acc: 0.8487 - precision: 0.6825 - recall: 0.4217 - f1_score: 0.5116 - val_loss: 0.3202 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 140/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3136 - acc: 0.8487 - precision: 0.6487 - recall: 0.4336 - f1_score: 0.5134 - val_loss: 0.3204 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 141/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3134 - acc: 0.8499 - precision: 0.6536 - recall: 0.4310 - f1_score: 0.5133 - val_loss: 0.3223 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 142/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3133 - acc: 0.8499 - precision: 0.6639 - recall: 0.4551 - f1_score: 0.5294 - val_loss: 0.3229 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 143/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3128 - acc: 0.8499 - precision: 0.6498 - recall: 0.4339 - f1_score: 0.5146 - val_loss: 0.3228 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 144/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3128 - acc: 0.8511 - precision: 0.6529 - recall: 0.4462 - f1_score: 0.5220 - val_loss: 0.3201 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 145/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3121 - acc: 0.8511 - precision: 0.6860 - recall: 0.4418 - f1_score: 0.5246 - val_loss: 0.3216 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 146/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3117 - acc: 0.8511 - precision: 0.6580 - recall: 0.4540 - f1_score: 0.5349 - val_loss: 0.3219 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 147/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.3113 - acc: 0.8499 - precision: 0.6526 - recall: 0.4502 - f1_score: 0.5187 - val_loss: 0.3194 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 148/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3110 - acc: 0.8499 - precision: 0.6417 - recall: 0.4223 - f1_score: 0.5051 - val_loss: 0.3168 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 149/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3106 - acc: 0.8487 - precision: 0.6522 - recall: 0.4294 - f1_score: 0.5118 - val_loss: 0.3168 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 150/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3103 - acc: 0.8499 - precision: 0.6767 - recall: 0.4302 - f1_score: 0.5182 - val_loss: 0.3177 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 151/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3102 - acc: 0.8511 - precision: 0.6636 - recall: 0.4354 - f1_score: 0.5243 - val_loss: 0.3190 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 152/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3098 - acc: 0.8511 - precision: 0.6687 - recall: 0.4391 - f1_score: 0.5251 - val_loss: 0.3177 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 153/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3096 - acc: 0.8499 - precision: 0.6636 - recall: 0.4276 - f1_score: 0.5175 - val_loss: 0.3146 - val_acc: 0.8517 - val_precision: 0.4796 - val_recall: 0.2919 - val_f1_score: 0.3557\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833/833 [==============================] - 0s 28us/step - loss: 0.3092 - acc: 0.8499 - precision: 0.6566 - recall: 0.4360 - f1_score: 0.5166 - val_loss: 0.3149 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 155/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3087 - acc: 0.8499 - precision: 0.6473 - recall: 0.4295 - f1_score: 0.5130 - val_loss: 0.3151 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 156/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3083 - acc: 0.8499 - precision: 0.6501 - recall: 0.4272 - f1_score: 0.5058 - val_loss: 0.3163 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 157/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3082 - acc: 0.8511 - precision: 0.6656 - recall: 0.4459 - f1_score: 0.5314 - val_loss: 0.3171 - val_acc: 0.8517 - val_precision: 0.4612 - val_recall: 0.3225 - val_f1_score: 0.3755\n",
      "Epoch 158/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3080 - acc: 0.8547 - precision: 0.6728 - recall: 0.4599 - f1_score: 0.5426 - val_loss: 0.3156 - val_acc: 0.8517 - val_precision: 0.4612 - val_recall: 0.3225 - val_f1_score: 0.3755\n",
      "Epoch 159/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3075 - acc: 0.8499 - precision: 0.6567 - recall: 0.4344 - f1_score: 0.5188 - val_loss: 0.3145 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 160/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3073 - acc: 0.8499 - precision: 0.6621 - recall: 0.4343 - f1_score: 0.5224 - val_loss: 0.3128 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 161/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3068 - acc: 0.8499 - precision: 0.6566 - recall: 0.4283 - f1_score: 0.5131 - val_loss: 0.3123 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 162/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3066 - acc: 0.8499 - precision: 0.6714 - recall: 0.4469 - f1_score: 0.5249 - val_loss: 0.3126 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 163/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3063 - acc: 0.8499 - precision: 0.6592 - recall: 0.4358 - f1_score: 0.5203 - val_loss: 0.3118 - val_acc: 0.8469 - val_precision: 0.4408 - val_recall: 0.2919 - val_f1_score: 0.3483\n",
      "Epoch 164/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3059 - acc: 0.8499 - precision: 0.6647 - recall: 0.4365 - f1_score: 0.5229 - val_loss: 0.3124 - val_acc: 0.8517 - val_precision: 0.4612 - val_recall: 0.3225 - val_f1_score: 0.3755\n",
      "Epoch 165/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3055 - acc: 0.8499 - precision: 0.6552 - recall: 0.4307 - f1_score: 0.5187 - val_loss: 0.3145 - val_acc: 0.8517 - val_precision: 0.4612 - val_recall: 0.3225 - val_f1_score: 0.3755\n",
      "Epoch 166/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3057 - acc: 0.8511 - precision: 0.6524 - recall: 0.4758 - f1_score: 0.5446 - val_loss: 0.3151 - val_acc: 0.8421 - val_precision: 0.4174 - val_recall: 0.3225 - val_f1_score: 0.3617\n",
      "Epoch 167/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3051 - acc: 0.8511 - precision: 0.6617 - recall: 0.4656 - f1_score: 0.5382 - val_loss: 0.3111 - val_acc: 0.8517 - val_precision: 0.4612 - val_recall: 0.3225 - val_f1_score: 0.3755\n",
      "Epoch 168/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3061 - acc: 0.8511 - precision: 0.6877 - recall: 0.4283 - f1_score: 0.5174 - val_loss: 0.3063 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 169/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3053 - acc: 0.8499 - precision: 0.6827 - recall: 0.4502 - f1_score: 0.5187 - val_loss: 0.3111 - val_acc: 0.8517 - val_precision: 0.4612 - val_recall: 0.3225 - val_f1_score: 0.3755\n",
      "Epoch 170/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 0.3040 - acc: 0.8523 - precision: 0.6714 - recall: 0.4411 - f1_score: 0.5297 - val_loss: 0.3104 - val_acc: 0.8517 - val_precision: 0.4612 - val_recall: 0.3225 - val_f1_score: 0.3755\n",
      "Epoch 171/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.3038 - acc: 0.8535 - precision: 0.6638 - recall: 0.4407 - f1_score: 0.5282 - val_loss: 0.3112 - val_acc: 0.8517 - val_precision: 0.4612 - val_recall: 0.3225 - val_f1_score: 0.3755\n",
      "Epoch 172/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3034 - acc: 0.8535 - precision: 0.6586 - recall: 0.4441 - f1_score: 0.5270 - val_loss: 0.3099 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 173/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3032 - acc: 0.8523 - precision: 0.6621 - recall: 0.4485 - f1_score: 0.5242 - val_loss: 0.3082 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 174/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3025 - acc: 0.8523 - precision: 0.6639 - recall: 0.4420 - f1_score: 0.5252 - val_loss: 0.3099 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 175/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3021 - acc: 0.8523 - precision: 0.6444 - recall: 0.4452 - f1_score: 0.5163 - val_loss: 0.3091 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 176/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.3016 - acc: 0.8523 - precision: 0.6458 - recall: 0.4473 - f1_score: 0.5222 - val_loss: 0.3105 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 177/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3011 - acc: 0.8535 - precision: 0.6679 - recall: 0.4591 - f1_score: 0.5392 - val_loss: 0.3081 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 178/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3006 - acc: 0.8523 - precision: 0.6905 - recall: 0.4478 - f1_score: 0.5392 - val_loss: 0.3063 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 179/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3002 - acc: 0.8499 - precision: 0.6474 - recall: 0.4233 - f1_score: 0.5099 - val_loss: 0.3064 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 180/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.3003 - acc: 0.8511 - precision: 0.6471 - recall: 0.4184 - f1_score: 0.5033 - val_loss: 0.3034 - val_acc: 0.8565 - val_precision: 0.5000 - val_recall: 0.2919 - val_f1_score: 0.3629\n",
      "Epoch 181/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.3005 - acc: 0.8487 - precision: 0.6633 - recall: 0.4419 - f1_score: 0.5182 - val_loss: 0.3070 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 182/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.2992 - acc: 0.8511 - precision: 0.6597 - recall: 0.4480 - f1_score: 0.5295 - val_loss: 0.3045 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 183/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.2989 - acc: 0.8487 - precision: 0.6584 - recall: 0.4310 - f1_score: 0.5180 - val_loss: 0.3039 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 184/200\n",
      "833/833 [==============================] - 0s 28us/step - loss: 0.2988 - acc: 0.8535 - precision: 0.6650 - recall: 0.4552 - f1_score: 0.5371 - val_loss: 0.3064 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 185/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.2983 - acc: 0.8523 - precision: 0.6686 - recall: 0.4483 - f1_score: 0.5311 - val_loss: 0.3064 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 186/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.2976 - acc: 0.8523 - precision: 0.6698 - recall: 0.4632 - f1_score: 0.5373 - val_loss: 0.3036 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 187/200\n",
      "833/833 [==============================] - 0s 30us/step - loss: 0.2974 - acc: 0.8499 - precision: 0.6442 - recall: 0.4255 - f1_score: 0.5083 - val_loss: 0.3008 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 188/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.2971 - acc: 0.8523 - precision: 0.6797 - recall: 0.4294 - f1_score: 0.5228 - val_loss: 0.3012 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 189/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.2966 - acc: 0.8499 - precision: 0.6635 - recall: 0.4160 - f1_score: 0.5067 - val_loss: 0.3021 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 190/200\n",
      "833/833 [==============================] - 0s 32us/step - loss: 0.2962 - acc: 0.8523 - precision: 0.6629 - recall: 0.4541 - f1_score: 0.5295 - val_loss: 0.3042 - val_acc: 0.8565 - val_precision: 0.4817 - val_recall: 0.3225 - val_f1_score: 0.3833\n",
      "Epoch 191/200\n",
      "833/833 [==============================] - 0s 29us/step - loss: 0.2961 - acc: 0.8547 - precision: 0.6714 - recall: 0.4624 - f1_score: 0.5414 - val_loss: 0.3049 - val_acc: 0.8517 - val_precision: 0.4558 - val_recall: 0.3225 - val_f1_score: 0.3769\n",
      "Epoch 192/200\n",
      "833/833 [==============================] - 0s 31us/step - loss: 0.2957 - acc: 0.8523 - precision: 0.6630 - recall: 0.4616 - f1_score: 0.5316 - val_loss: 0.3020 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 193/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.2951 - acc: 0.8511 - precision: 0.6503 - recall: 0.4452 - f1_score: 0.5192 - val_loss: 0.3011 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 194/200\n",
      "833/833 [==============================] - 0s 34us/step - loss: 0.2949 - acc: 0.8511 - precision: 0.6640 - recall: 0.4498 - f1_score: 0.5321 - val_loss: 0.3007 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 195/200\n",
      "833/833 [==============================] - 0s 35us/step - loss: 0.2948 - acc: 0.8523 - precision: 0.6663 - recall: 0.4125 - f1_score: 0.5015 - val_loss: 0.2982 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 196/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.2941 - acc: 0.8523 - precision: 0.6761 - recall: 0.4434 - f1_score: 0.5310 - val_loss: 0.3005 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 197/200\n",
      "833/833 [==============================] - 0s 36us/step - loss: 0.2937 - acc: 0.8523 - precision: 0.6581 - recall: 0.4402 - f1_score: 0.5230 - val_loss: 0.3003 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 198/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.2933 - acc: 0.8523 - precision: 0.6493 - recall: 0.4435 - f1_score: 0.5235 - val_loss: 0.2996 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 199/200\n",
      "833/833 [==============================] - 0s 37us/step - loss: 0.2930 - acc: 0.8547 - precision: 0.6900 - recall: 0.4446 - f1_score: 0.5314 - val_loss: 0.2972 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n",
      "Epoch 200/200\n",
      "833/833 [==============================] - 0s 33us/step - loss: 0.2927 - acc: 0.8559 - precision: 0.7003 - recall: 0.4386 - f1_score: 0.5337 - val_loss: 0.2982 - val_acc: 0.8612 - val_precision: 0.5204 - val_recall: 0.3225 - val_f1_score: 0.3907\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperparameters['architecture'] = get_CNN_1C\n",
    "hyperparameters['lr'] = 0.0001\n",
    "hyperparameters['dropout_conv'] = 0\n",
    "hyperparameters['dropout_dense'] = 0\n",
    "hyperparameters['epochs'] = 200\n",
    "\n",
    "model, history = train_model(hyperparameters, custom_metrics,\n",
    "                             X_train, Y_train_als, X_dev, Y_dev_als)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bigger network seems to improve the new metrics, even without tuning any parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(disease):\n",
    "    in_dir = os.path.join(rootdir, 'trained_models')\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel('loss')\n",
    "    plt.grid()\n",
    "\n",
    "    for filename in os.listdir(in_dir):\n",
    "        if filename.startswith(disease) and filename.endswith('pickle'):\n",
    "            with open(os.path.join(in_dir, filename), 'rb') as handle:\n",
    "                history = pickle.load(handle)\n",
    "            ax.plot(history['val_loss'], label=filename[:7])\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
